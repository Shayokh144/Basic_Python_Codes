{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import defaultdict\n",
    "from nltk.translate import AlignedSent\n",
    "from nltk.translate import Alignment\n",
    "from nltk.translate import IBMModel\n",
    "from nltk.translate.ibm_model import Counts\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IBMModel1(IBMModel):\n",
    "    def __init__(self, sentence_aligned_corpus, iterations, probability_tables=None):\n",
    "        \"\"\"\n",
    "        Train on ``sentence_aligned_corpus`` and create a lexical\n",
    "        translation model.\n",
    "\n",
    "        Translation direction is from ``AlignedSent.mots`` to\n",
    "        ``AlignedSent.words``.\n",
    "\n",
    "        :param sentence_aligned_corpus: Sentence-aligned parallel corpus\n",
    "        :type sentence_aligned_corpus: list(AlignedSent)\n",
    "\n",
    "        :param iterations: Number of iterations to run training algorithm\n",
    "        :type iterations: int\n",
    "\n",
    "        :param probability_tables: Optional. Use this to pass in custom\n",
    "            probability values. If not specified, probabilities will be\n",
    "            set to a uniform distribution, or some other sensible value.\n",
    "            If specified, the following entry must be present:\n",
    "            ``translation_table``.\n",
    "            See ``IBMModel`` for the type and purpose of this table.\n",
    "        :type probability_tables: dict[str]: object\n",
    "        \"\"\"\n",
    "        super(IBMModel1, self).__init__(sentence_aligned_corpus)\n",
    "\n",
    "        if probability_tables is None:\n",
    "            self.set_uniform_probabilities(sentence_aligned_corpus)\n",
    "        else:\n",
    "            # Set user-defined probabilities\n",
    "            self.translation_table = probability_tables['translation_table']\n",
    "\n",
    "        for n in range(0, iterations):\n",
    "            self.train(sentence_aligned_corpus)\n",
    "\n",
    "        self.align_all(sentence_aligned_corpus)\n",
    "\n",
    "    def set_uniform_probabilities(self, sentence_aligned_corpus):\n",
    "        initial_prob = 1 / len(self.trg_vocab)\n",
    "        if initial_prob < IBMModel.MIN_PROB:\n",
    "            warnings.warn(\n",
    "                \"Target language vocabulary is too large (\"\n",
    "                + str(len(self.trg_vocab))\n",
    "                + \" words). \"\n",
    "                \"Results may be less accurate.\"\n",
    "            )\n",
    "\n",
    "        for t in self.trg_vocab:\n",
    "            self.translation_table[t] = defaultdict(lambda: initial_prob)\n",
    "\n",
    "\n",
    "    def train(self, parallel_corpus):\n",
    "        counts = Counts()\n",
    "        for aligned_sentence in parallel_corpus:\n",
    "            trg_sentence = aligned_sentence.words\n",
    "            src_sentence = [None] + aligned_sentence.mots\n",
    "\n",
    "            # E step (a): Compute normalization factors to weigh counts\n",
    "            total_count = self.prob_all_alignments(src_sentence, trg_sentence)\n",
    "\n",
    "            # E step (b): Collect counts\n",
    "            for t in trg_sentence:\n",
    "                for s in src_sentence:\n",
    "                    count = self.prob_alignment_point(s, t)\n",
    "                    normalized_count = count / total_count[t]\n",
    "                    counts.t_given_s[t][s] += normalized_count\n",
    "                    counts.any_t_given_s[s] += normalized_count\n",
    "\n",
    "        # M step: Update probabilities with maximum likelihood estimate\n",
    "        self.maximize_lexical_translation_probabilities(counts)\n",
    "\n",
    "\n",
    "    def prob_all_alignments(self, src_sentence, trg_sentence):\n",
    "        \"\"\"\n",
    "        Computes the probability of all possible word alignments,\n",
    "        expressed as a marginal distribution over target words t\n",
    "\n",
    "        Each entry in the return value represents the contribution to\n",
    "        the total alignment probability by the target word t.\n",
    "\n",
    "        To obtain probability(alignment | src_sentence, trg_sentence),\n",
    "        simply sum the entries in the return value.\n",
    "\n",
    "        :return: Probability of t for all s in ``src_sentence``\n",
    "        :rtype: dict(str): float\n",
    "        \"\"\"\n",
    "        alignment_prob_for_t = defaultdict(lambda: 0.0)\n",
    "        for t in trg_sentence:\n",
    "            for s in src_sentence:\n",
    "                alignment_prob_for_t[t] += self.prob_alignment_point(s, t)\n",
    "        return alignment_prob_for_t\n",
    "\n",
    "\n",
    "    def prob_alignment_point(self, s, t):\n",
    "        \"\"\"\n",
    "        Probability that word ``t`` in the target sentence is aligned to\n",
    "        word ``s`` in the source sentence\n",
    "        \"\"\"\n",
    "        return self.translation_table[t][s]\n",
    "\n",
    "\n",
    "    def prob_t_a_given_s(self, alignment_info):\n",
    "        \"\"\"\n",
    "        Probability of target sentence and an alignment given the\n",
    "        source sentence\n",
    "        \"\"\"\n",
    "        prob = 1.0\n",
    "\n",
    "        for j, i in enumerate(alignment_info.alignment):\n",
    "            if j == 0:\n",
    "                continue  # skip the dummy zeroeth element\n",
    "            trg_word = alignment_info.trg_sentence[j]\n",
    "            src_word = alignment_info.src_sentence[i]\n",
    "            prob *= self.translation_table[trg_word][src_word]\n",
    "\n",
    "        return max(prob, IBMModel.MIN_PROB)\n",
    "\n",
    "\n",
    "    def align_all(self, parallel_corpus):\n",
    "        for sentence_pair in parallel_corpus:\n",
    "            self.align(sentence_pair)\n",
    "\n",
    "\n",
    "    def align(self, sentence_pair):\n",
    "        \"\"\"\n",
    "        Determines the best word alignment for one sentence pair from\n",
    "        the corpus that the model was trained on.\n",
    "\n",
    "        The best alignment will be set in ``sentence_pair`` when the\n",
    "        method returns. In contrast with the internal implementation of\n",
    "        IBM models, the word indices in the ``Alignment`` are zero-\n",
    "        indexed, not one-indexed.\n",
    "\n",
    "        :param sentence_pair: A sentence in the source language and its\n",
    "            counterpart sentence in the target language\n",
    "        :type sentence_pair: AlignedSent\n",
    "        \"\"\"\n",
    "        best_alignment = []\n",
    "\n",
    "        for j, trg_word in enumerate(sentence_pair.words):\n",
    "            # Initialize trg_word to align with the NULL token\n",
    "            best_prob = max(self.translation_table[trg_word][None], IBMModel.MIN_PROB)\n",
    "            best_alignment_point = None\n",
    "            for i, src_word in enumerate(sentence_pair.mots):\n",
    "                align_prob = self.translation_table[trg_word][src_word]\n",
    "                if align_prob >= best_prob:  # prefer newer word in case of tie\n",
    "                    best_prob = align_prob\n",
    "                    best_alignment_point = i\n",
    "\n",
    "            best_alignment.append((j, best_alignment_point))\n",
    "\n",
    "        sentence_pair.alignment = Alignment(best_alignment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "limit = 10000\n",
    "def ReadData(fileName):\n",
    "    i=0\n",
    "    tokenizeDataList = []\n",
    "    with open(fileName,\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            temp = line.split()\n",
    "            tokenizeDataList.append(temp)\n",
    "            i+=1\n",
    "            if i == limit:\n",
    "                break\n",
    "    return tokenizeDataList\n",
    "\n",
    "germanDataList = ReadData(\"europarl-v7.de-en.de\")\n",
    "print(len(germanDataList))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Die',\n",
       " 'Stadt,',\n",
       " 'in',\n",
       " 'der',\n",
       " 'ich',\n",
       " 'lebe,',\n",
       " 'ist',\n",
       " 'sehr',\n",
       " 'klein.',\n",
       " 'Es',\n",
       " 'hat',\n",
       " 'nur',\n",
       " '4500',\n",
       " 'Einwohner',\n",
       " 'und',\n",
       " 'ist',\n",
       " 'von',\n",
       " 'Ackerland',\n",
       " 'und',\n",
       " 'Wäldern',\n",
       " 'umgeben.',\n",
       " 'Wir',\n",
       " 'haben',\n",
       " 'auch',\n",
       " 'einen',\n",
       " 'schönen',\n",
       " 'See,',\n",
       " 'der',\n",
       " 'eine',\n",
       " 'große',\n",
       " 'Attraktion',\n",
       " 'des',\n",
       " 'Sommers',\n",
       " 'ist,',\n",
       " 'wenn',\n",
       " 'viele',\n",
       " 'Touristen',\n",
       " 'in',\n",
       " 'den',\n",
       " 'Urlaub',\n",
       " 'kommen.',\n",
       " 'Ich',\n",
       " 'bin',\n",
       " 'im',\n",
       " 'Winter',\n",
       " 'auf',\n",
       " 'dem',\n",
       " 'See',\n",
       " 'Schlittschuh',\n",
       " 'gefahren,',\n",
       " 'als',\n",
       " 'ich',\n",
       " 'jünger',\n",
       " 'war',\n",
       " 'und',\n",
       " 'die',\n",
       " 'Winter',\n",
       " 'kälter',\n",
       " 'waren.',\n",
       " 'Sie',\n",
       " 'können',\n",
       " 'dort',\n",
       " 'schwimmen,',\n",
       " 'baden',\n",
       " 'oder',\n",
       " 'windsurfen',\n",
       " 'und',\n",
       " 'sogar',\n",
       " 'Wakeboarden',\n",
       " 'lernen.',\n",
       " 'Meine',\n",
       " 'Stadt',\n",
       " 'ist',\n",
       " 'nicht',\n",
       " 'weit',\n",
       " 'von',\n",
       " 'der',\n",
       " 'Großstadt',\n",
       " 'entfernt,',\n",
       " 'so',\n",
       " 'dass',\n",
       " 'die',\n",
       " 'Leute',\n",
       " 'dort',\n",
       " 'auch',\n",
       " 'einkaufen',\n",
       " 'können.',\n",
       " 'Mit',\n",
       " 'dem',\n",
       " 'Zug',\n",
       " 'sind',\n",
       " 'es',\n",
       " 'nur',\n",
       " '15',\n",
       " 'Minuten.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "germanDataList[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "englishDataList = ReadData(\"europarl-v7.de-en.en\")\n",
    "print(len(englishDataList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#englishDataList[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainData = []\n",
    "for g,e in zip(germanDataList, englishDataList):\n",
    "    trainData.append(AlignedSent(g, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ibm1 = IBMModel1(trainData[:10000], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'town',\n",
       " 'I',\n",
       " 'live',\n",
       " 'in',\n",
       " 'is',\n",
       " 'quiet',\n",
       " 'small',\n",
       " '.',\n",
       " 'It',\n",
       " 'only',\n",
       " 'has',\n",
       " '4500',\n",
       " 'inhabitants',\n",
       " 'and',\n",
       " 'is',\n",
       " 'surrounded',\n",
       " 'by',\n",
       " 'farmland',\n",
       " 'and',\n",
       " 'forests.',\n",
       " 'we',\n",
       " 'also',\n",
       " 'have',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'lake',\n",
       " 'which',\n",
       " 'is',\n",
       " 'great',\n",
       " 'attraction',\n",
       " 'of',\n",
       " 'summer',\n",
       " 'when',\n",
       " 'many',\n",
       " 'tourists',\n",
       " 'come',\n",
       " 'for',\n",
       " 'holidays.I',\n",
       " 'used',\n",
       " 'to',\n",
       " 'ice',\n",
       " 'skate',\n",
       " 'on',\n",
       " 'the',\n",
       " 'lake',\n",
       " 'in',\n",
       " 'winter',\n",
       " 'when',\n",
       " 'I',\n",
       " 'was',\n",
       " 'younger',\n",
       " 'and',\n",
       " 'winters',\n",
       " 'were',\n",
       " 'colder.',\n",
       " 'You',\n",
       " 'can',\n",
       " 'go',\n",
       " 'swimming,',\n",
       " 'salling',\n",
       " 'or',\n",
       " 'windsurfing',\n",
       " 'there',\n",
       " 'and',\n",
       " 'you',\n",
       " 'even',\n",
       " 'can',\n",
       " 'learn',\n",
       " 'how',\n",
       " 'to',\n",
       " 'wakeboard.',\n",
       " 'My',\n",
       " 'town',\n",
       " 'is',\n",
       " 'not',\n",
       " 'far',\n",
       " 'from',\n",
       " 'big',\n",
       " 'city',\n",
       " 'so',\n",
       " 'people',\n",
       " 'can',\n",
       " 'go',\n",
       " 'shopping',\n",
       " 'there',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'it',\n",
       " 'only',\n",
       " 'takes',\n",
       " '15',\n",
       " 'minutes',\n",
       " 'by',\n",
       " 'train.Resumption',\n",
       " 'of',\n",
       " 'the',\n",
       " 'session']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = trainData[0]\n",
    "test_sentence.mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018794825765294416\n"
     ]
    }
   ],
   "source": [
    "print(ibm1.translation_table['Stadt']['town'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
